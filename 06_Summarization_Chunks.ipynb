{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoiH_ANAgzrI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "xxJ2iMxFhk60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_fi='/content/drive/MyDrive/CBS/CBS Thesis Lydia & Sara/Data/04_Data_Analysis/20k_finnish_predicted.csv'\n",
        "path_se='/content/drive/MyDrive/CBS/CBS Thesis Lydia & Sara/Data/04_Data_Analysis/20k_swedish_predicted.csv'\n",
        "path_de='/content/drive/MyDrive/CBS/CBS Thesis Lydia & Sara/Data/04_Data_Analysis/20k_german_predicted.csv'\n",
        "path_en='/content/drive/MyDrive/CBS/CBS Thesis Lydia & Sara/Data/04_Data_Analysis/20k_english_predicted.csv'\n",
        "path_da='/content/drive/MyDrive/CBS/CBS Thesis Lydia & Sara/Data/04_Data_Analysis/20k_danish_predicted.csv'\n",
        "\n",
        "df= pd.read_csv(path_XX, sep='\\t', encoding='utf-16') #set to current language\n",
        "\n",
        "df['predicted_label'] = df['predicted_label'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "brUftcsug9xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_names = [\n",
        "    'Not Categorisable',\n",
        "    'Conflict & Crisis', #1\n",
        "    'Migration Flow', #2\n",
        "    'Host Country Security', #3\n",
        "    'Host Country Politics', #4\n",
        "    'Refugee Rights & Advocacy', #5\n",
        "    'Host Country Resources', #6\n",
        "    'Host Country Symbolic Discourse' #7\n",
        "]\n",
        "\n",
        "filtered_dfs = {}\n",
        "\n",
        "#Creating separate dataframes for each label\n",
        "for i, label_name in enumerate(label_names):\n",
        "    filtered_dfs[label_name] = df[df['predicted_label'].apply(lambda x: x[i] == 1)]\n",
        "\n",
        "filtered_dfs['No Label'] = df[df['predicted_label'].apply(lambda x: all(i == 0 for i in (x)))]"
      ],
      "metadata": {
        "id": "E8pCbn7PhiaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concatenated_texts={}\n",
        "\n",
        "for label in label_names:\n",
        "    #Excluding all tweets that belong to Not Categorisable or are predicted to no categories from the summarization files\n",
        "    if label in ['No Label', 'Not Categorisable']:\n",
        "        continue\n",
        "\n",
        "    concatenated_texts[label] = ' \\n '.join(filtered_dfs[label]['Translation'])\n"
      ],
      "metadata": {
        "id": "clwF4mMth3Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = '/content/drive/MyDrive/CBS/CBS Thesis Lydia & Sara/Data/06_ZIP_V2/XX/' #set to current country\n",
        "\n",
        "counter_frame = 1\n",
        "\n",
        "for label, text in concatenated_texts.items():\n",
        "    print(f\"Processing label: {label}\")\n",
        "    chunk_position = 0\n",
        "    counter_chunk = 1\n",
        "\n",
        "    label_directory = os.path.join(directory_path, f\"L0{counter_frame}\")\n",
        "    os.makedirs(label_directory, exist_ok=True)\n",
        "\n",
        "    while chunk_position < len(text):\n",
        "\n",
        "        #Using 19500 characters as an approximation of a suitable chunk size\n",
        "        end_position = chunk_position + 19500\n",
        "\n",
        "        # Adjust the chunk size to end on a newline\n",
        "        if end_position < len(text):\n",
        "          end_position = text.rfind('\\n', chunk_position, end_position) + 1\n",
        "\n",
        "        if end_position == 0:\n",
        "                end_position = chunk_position + 19500\n",
        "\n",
        "        # Extract the chunk and adjust chunk_position\n",
        "        chunk = text[chunk_position:end_position]\n",
        "        chunk_position = end_position\n",
        "\n",
        "        # Define the file path and write the chunk to a file\n",
        "        filename = f\"{counter_chunk}.txt\"\n",
        "        full_path = os.path.join(label_directory, filename)\n",
        "        with open(full_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(chunk)\n",
        "\n",
        "        print(f\"Chunk {counter_chunk} written to {full_path}\")\n",
        "        counter_chunk += 1\n",
        "\n",
        "    counter_frame += 1\n"
      ],
      "metadata": {
        "id": "5gtmkD-d8zCY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}