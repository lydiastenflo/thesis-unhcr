{"cells":[{"cell_type":"code","source":["#!pip install emoji"],"metadata":{"id":"hdGu9hfssvrw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715597780733,"user_tz":-120,"elapsed":26454,"user":{"displayName":"Sara Kautto","userId":"16026699756288569120"}},"outputId":"67e48e5e-857f-4fc7-992c-3eaedf2eb223"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting emoji\n","  Downloading emoji-2.11.1-py2.py3-none-any.whl (433 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: emoji\n","Successfully installed emoji-2.11.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AAw2E_JUtUzm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715597853315,"user_tz":-120,"elapsed":72585,"user":{"displayName":"Sara Kautto","userId":"16026699756288569120"}},"outputId":"d50f033c-c09d-4571-ee67-42e73c39b26d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fVB_Ra2stF4e"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","import emoji"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"B9RfkaM02cxv"},"outputs":[],"source":["current_language='-' #Set to current language\n","path = '/' #Change to folder path\n","\n","\n","#File paths: Adjust to relevant paths\n","file1 = '/en/refugee_OR_refugees_OR_refugees_OR_refugees_OR_Ref - Feb 27, 2024 - 3 27 39 PM.xlsx'\n","file2 = '/en/refugee_OR_refugees_OR_refugees_OR_refugees_OR_Ref - Feb 27, 2024 - 3 30 39 PM.xlsx'\n","file3 = '/en/refugee_OR_refugees_OR_refugees_OR_refugees_OR_Ref - Feb 27, 2024 - 3 35 31 PM.xlsx'\n","file4 = '/en/refugee_OR_refugees_OR_refugees_OR_refugees_OR_Ref - Feb 27, 2024 - 3 39 03 PM.xlsx'\n","file5 = '/en/refugee_OR_refugees_OR_refugees_OR_refugees_OR_Ref - Feb 27, 2024 - 3 43 56 PM.xlsx'\n","\n","\n","data = [file1, file2, file3, file4, file5]\n","\n","df = pd.concat((pd.read_excel(path+file) for file in data), ignore_index=True)"]},{"cell_type":"markdown","metadata":{"id":"qMxpudnHbAp0"},"source":["# Clean data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J_Mp8KKDB_F6"},"outputs":[],"source":["#FILTER\n","# Keep rows where language is the current language\n","df = df[df['Language'] == current_language]\n","\n","# Keep rows where source is 'Twitter'\n","df = df[df['Source'] == 'Twitter']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0NW_4TYIMoL-"},"outputs":[],"source":["#DUPLICATES\n","# Remove duplicate rows\n","df = df.drop_duplicates()\n","\n","# Remove url duplicates\n","df = df.drop_duplicates(subset=['Hit Sentence'])\n","\n","# Drop rows with NaN values in 'Hit Sentence' column from the original DataFrame\n","df = df.dropna(subset=['Hit Sentence'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DrBCIclfexXe"},"outputs":[],"source":["# Columns to drop\n","columns_to_drop = [\n","    'Social Echo Total', 'Editorial Echo', 'Twitter Social Echo', 'Facebook Social Echo', 'Reddit Social Echo',\n","    'Views', 'Likes', 'Replies', 'Retweets', 'Comments', 'Shares', 'Reactions', 'Engagement', 'Estimated Views',\n","    'Desktop Reach', 'Mobile Reach', 'National Viewership', 'AVE', 'Reach', 'Headline', 'Opening Text', 'Source', 'Document Tags',\n","    'Twitter Authority', 'Twitter Followers', 'Twitter Following', 'Twitter Id', 'Twitter Client', 'Twitter Screen Name', 'Twitter Bio',\n","    'Country', 'State', 'City', 'Subregion', 'Influencer', 'User Profile Url', 'Threads', 'Is Verified',\n","    'Tweet Id', 'Parent URL', 'Key Phrases', 'Document ID',\n","    'Input Name', 'Keywords', 'Alternate Date Format', 'Time'\n","]\n","\n","# Create a clean DataFrame by dropping specified columns\n","df_clean = df.drop(columns=columns_to_drop)\n"]},{"cell_type":"markdown","metadata":{"id":"2Py5tv2pS5lQ"},"source":["# Text processing - mentions, emojis and URLs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zvh7-tyIS9-t"},"outputs":[],"source":["df_new =  df_clean[:]\n","\n","# Replace mentions starting with '@' with '@USER'\n","df_new['Hit Sentence'] = df_new['Hit Sentence'].str.replace(r'@\\w+', '@USER')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TYe7gdN1UP2g"},"outputs":[],"source":["# Function to transform emojis into text\n","def transform_emojis(text):\n","    return emoji.demojize(text)\n","\n","# Apply the function to the 'Hit Sentence' column\n","df_new['Hit Sentence'] = df_new['Hit Sentence'].apply(transform_emojis)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDVz-1TUxlIn"},"outputs":[],"source":["# Define a regex pattern to match URLs\n","url_pattern = r'https?://\\S+|www\\.\\S+'\n","\n","# Function to replace URLs with 'URL'\n","def replace_urls_with_url(text):\n","    return re.sub(url_pattern, 'URL', text)\n","\n","# Apply the function\n","df_new['Hit Sentence'] = df_new['Hit Sentence'].apply(replace_urls_with_url)"]},{"cell_type":"markdown","metadata":{"id":"DNZoIfmSOle2"},"source":["# Save cleaned datasets"]},{"cell_type":"markdown","source":["Create a Sample dataset for the labelling process"],"metadata":{"id":"20mR8uExnFSN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALViXkr4tgkn"},"outputs":[],"source":["# Shuffle the rows in df_test\n","df_shuffled = df_new.sample(frac=1, random_state=42)\n","\n","# Reset the index of the shuffled DataFrame\n","df_shuffled.reset_index(drop=True, inplace=True)\n","df_shuffled.head()\n","\n","# Sample 400 random rows per languege\n","df_sampled = df_shuffled.sample(n=400, random_state=42)\n","df_sampled.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sBhFQF3cu9lg"},"outputs":[],"source":["# Save the sampled DataFrame to a CSV file\n","df_sampled.to_csv('/path/df.csv', sep='\\t', encoding='utf-16', index=False)"]},{"cell_type":"markdown","source":["Saving the non-sampled tweets to another file:"],"metadata":{"id":"S6lesT5oMQdF"}},{"cell_type":"code","source":["df_new = df_new[~df_new['URL'].isin(df_sampled['URL'])]\n","\n","# Save the DataFrame to a CSV file in Google Drive\n","df_new.to_csv('/path/df.csv', sep='\\t', encoding='utf-16', index=False)"],"metadata":{"id":"_PkzqiIaMPE2"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}