{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpqrG2vEENMl",
        "outputId": "cfb571ed-0f62-423a-d2b7-03607d912882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPVmPPIWTg-J"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate==0.20.3 torch==2.2.1\n",
        "!pip install -q -U transformers peft bitsandbytes\n",
        "!pip install -q torch datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tAJ9ByQEvCW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os,torch\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    RobertaTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyKqOYtYEgOG"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zizl6SCRYeG7"
      },
      "source": [
        "# RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytbEXJ9swEw6"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "model= AutoModelForSequenceClassification.from_pretrained(\"FacebookAI/roberta-base\", num_labels=8,\n",
        "                                                          problem_type=\"multi_label_classification\")\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "model=model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI4NOzmNYa9N"
      },
      "source": [
        "\n",
        "\n",
        "# Mistral & Llama 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXJ09SrBn8ou"
      },
      "outputs": [],
      "source": [
        "# Qunatization Config\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True,\n",
        "    bnb_4bit_quant_type = 'nf4',\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_compute_dtype = torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nsFqjdcEiC1"
      },
      "outputs": [],
      "source": [
        "# LORA Config\n",
        "\n",
        "target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=128,\n",
        "    lora_alpha= 256,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    target_modules = target_modules,\n",
        "    task_type = 'SEQ_CLS',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHirSmFYEkv9"
      },
      "outputs": [],
      "source": [
        "mistral='mistralai/Mistral-7B-v0.1'\n",
        "llama='NousResearch/Llama-2-7b-hf'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(mistral)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(mistral,\n",
        "                                                           quantization_config=quantization_config,\n",
        "                                                           num_labels=8,\n",
        "                                                           torch_dtype=torch.float16,\n",
        "                                                           problem_type=\"multi_label_classification\")\n",
        "\n",
        "\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "model=model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainable Parameters"
      ],
      "metadata": {
        "id": "ti7AiCx4SSHm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QZb4oZUis1y"
      },
      "outputs": [],
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"Trainable model parameters: {trainable_model_params}\\nAll model parameters: {all_model_params}\\nPercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa-wjz7XY4DP"
      },
      "source": [
        "# Importing data & one-hot encoding the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2D_TMx4HEFA"
      },
      "outputs": [],
      "source": [
        "path='' #path to the labelled dataset\n",
        "df= pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "category_to_index = {'not_categorisable': 0,\n",
        "          'conflict_and_crisis': 1,\n",
        "          'migration_flow': 2,\n",
        "          'host_country_security': 3,\n",
        "          'host_country_politics': 4,\n",
        "          'refugee_rights_and_advocacy': 5,\n",
        "          'host_country_resources': 6,\n",
        "          'host_country_symbolic_discourse': 7}\n",
        "\n",
        "num_labels = len(category_to_index)\n",
        "\n",
        "def convert_labels(row):\n",
        "    label_array = [0] * num_labels\n",
        "    entry = row['refugee_sentiment']\n",
        "    try:\n",
        "        parsed_entry = json.loads(entry)\n",
        "        if 'choices' in parsed_entry:\n",
        "            categories = parsed_entry['choices']\n",
        "        else:\n",
        "            categories = [parsed_entry]\n",
        "    except json.JSONDecodeError:\n",
        "        categories = [entry]\n",
        "\n",
        "    #One-hot encoding the categories\n",
        "    for category in categories:\n",
        "        if category in category_to_index:\n",
        "            index = category_to_index[category]\n",
        "            label_array[index] = 1\n",
        "\n",
        "    return label_array\n",
        "\n",
        "\n",
        "df['one_hot'] = df.apply(convert_labels, axis=1)\n",
        "\n",
        "desired_columns = ['Translation', 'URL', 'Date', 'Language', 'Sentiment', 'one_hot']\n",
        "df = df[desired_columns]"
      ],
      "metadata": {
        "id": "QxsFDVrVUFuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the labelled data & creating the datasets"
      ],
      "metadata": {
        "id": "SBm9d8FcS-nV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1CyDPBCCUcx"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "class CustomHFDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_len, text_column, one_hot_column, device=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dataset = dataset\n",
        "        self.text_column = text_column\n",
        "        self.one_hot_column = one_hot_column\n",
        "        self.max_len = max_len\n",
        "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        record = self.dataset[index]\n",
        "        text = str(record[self.text_column])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        labels = record[self.one_hot_column]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].flatten(),\n",
        "            \"token_type_ids\": inputs.get(\"token_type_ids\", torch.tensor([])).flatten(),\n",
        "            \"labels\": torch.FloatTensor(labels)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv8vE7qkCeQp"
      },
      "outputs": [],
      "source": [
        "train_size = 0.8\n",
        "val_test_size = 0.5\n",
        "MAX_LEN = 512\n",
        "\n",
        "#Splitting into training and temporary data\n",
        "train_temp = df.sample(frac=train_size, random_state=42)\n",
        "temp = df.drop(train_temp.index)\n",
        "\n",
        "#Splitting temporary data into validation and test sets\n",
        "val = temp.sample(frac=val_test_size, random_state=42)\n",
        "test = temp.drop(val.index)\n",
        "\n",
        "# Resetting indices and creating datasets\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': Dataset.from_pandas(train_temp.reset_index(drop=True)),\n",
        "    'validation': Dataset.from_pandas(val.reset_index(drop=True)),\n",
        "    'test': Dataset.from_pandas(test.reset_index(drop=True))\n",
        "})\n",
        "\n",
        "custom_datasets = {}\n",
        "\n",
        "for phase in ['train', 'validation', 'test']:\n",
        "    dataset = dataset_dict[phase]\n",
        "    custom_datasets[phase] = CustomHFDataset(\n",
        "        dataset,\n",
        "        tokenizer,\n",
        "        max_len=MAX_LEN,\n",
        "        text_column='Translation',\n",
        "        one_hot_column='one_hot'\n",
        "    )\n",
        "\n",
        "encoded_dict = DatasetDict(custom_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPpWB7gCzSih"
      },
      "source": [
        "#Mutli-label classification adjustments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ4IePI9352y"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(predictions))\n",
        "\n",
        "    y_pred = np.zeros(probs.shape)\n",
        "    y_pred[np.where(probs >= threshold)] = 1\n",
        "    y_true = labels\n",
        "\n",
        "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
        "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
        "\n",
        "    # Return Metrics Dict\n",
        "    metrics = {'f1': f1_micro_average,\n",
        "               'roc_auc': roc_auc}\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "    result = multi_label_metrics(predictions=preds, labels=p.label_ids)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzrlWiJALvfA"
      },
      "outputs": [],
      "source": [
        "def custom_data_collator(features):\n",
        "    input_ids = torch.stack([f['input_ids'] for f in features])\n",
        "    batch = {'input_ids': input_ids}\n",
        "\n",
        "    attention_masks = torch.stack([f['attention_mask'] for f in features])\n",
        "    batch['attention_mask'] = attention_masks\n",
        "\n",
        "    if isinstance(features[0]['labels'], torch.Tensor):\n",
        "        labels = torch.stack([f['labels'] for f in features])\n",
        "        batch['labels'] = labels\n",
        "\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2dMae4p0Y8p"
      },
      "outputs": [],
      "source": [
        "def custom_loss_function(outputs, labels, penalty_factor=1.0):\n",
        "    bce_loss = F.binary_cross_entropy_with_logits(outputs, labels, reduction='none')\n",
        "\n",
        "    class_1_pred = torch.sigmoid(outputs[:, 0]) > 0.5\n",
        "    other_classes_pred = torch.sigmoid(outputs[:, 1:]) > 0.5\n",
        "\n",
        "    #Custom penalty\n",
        "    penalty_condition = class_1_pred.unsqueeze(-1) & other_classes_pred\n",
        "    penalties = penalty_condition.any(dim=1).float() * penalty_factor\n",
        "\n",
        "    # Apply penalty\n",
        "    enhanced_loss = bce_loss.mean(dim=1) + penalties\n",
        "\n",
        "    return enhanced_loss.mean()\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss = custom_loss_function(logits, labels.float())\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "jTF6TfMXjNCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=10\n",
        "batch_size = 16\n",
        "metric_name = \"f1\"\n",
        "\n",
        "\n",
        "batches_per_epoch = 1600 / batch_size\n",
        "total_training_steps = batches_per_epoch * epochs\n",
        "warmup_steps = total_training_steps * 0.01"
      ],
      "metadata": {
        "id": "ljiIuJA3jJmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(output_dir='', #path to output dir\n",
        "                          logging_dir='' , #path to logs\n",
        "                          remove_unused_columns=False ,\n",
        "                          num_train_epochs=epochs,\n",
        "                          load_best_model_at_end=True,\n",
        "                          evaluation_strategy = \"epoch\",\n",
        "                          save_strategy=\"epoch\",\n",
        "                          logging_steps=10,\n",
        "                          learning_rate=2e-5,\n",
        "                          metric_for_best_model=metric_name,\n",
        "                          per_device_train_batch_size= batch_size,\n",
        "                          per_device_eval_batch_size= batch_size,\n",
        "                          warmup_steps=int(warmup_steps),\n",
        "                          save_total_limit=2,\n",
        "                          weight_decay=0.001,\n",
        "                          max_grad_norm=1.0,\n",
        "\n",
        "                          #For QLORA training:\n",
        "                          #label_names=['labels'],\n",
        "                          #fp16=False,\n",
        "                          #bf16=False\n",
        "                          )\n",
        "\n",
        "trainer = CustomTrainer(model=model,\n",
        "                        args=args,\n",
        "                        train_dataset=encoded_dict['train'],\n",
        "                        eval_dataset=encoded_dict['validation'],\n",
        "                        data_collator=custom_data_collator,\n",
        "                        tokenizer = tokenizer,\n",
        "                        compute_metrics = compute_metrics,\n",
        "                        )\n"
      ],
      "metadata": {
        "id": "NWm2es7WjXK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAG1x4YB2dFu"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "eval_result = trainer.evaluate()\n",
        "print(eval_result)\n",
        "\n",
        "# Save the model and tokenizer\n",
        "save_path = \"\" #path to model\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulvCc71GgGmH"
      },
      "source": [
        "# Evaluate on Test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUPB90OtEkQV"
      },
      "outputs": [],
      "source": [
        "from peft import AutoPeftModelForSequenceClassification #For PEFT\n",
        "\n",
        "save_path = \"\" #set to current model\n",
        "\n",
        "#RoBERTA\n",
        "#model = AutoModelForSequenceClassification.from_pretrained(save_path, num_labels=8, problem_type=\"multi_label_classification\")\n",
        "\n",
        "#Mistral, LLama\n",
        "model = AutoPeftModelForSequenceClassification.from_pretrained(save_path, num_labels=8, problem_type=\"multi_label_classification\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
        "\n",
        "model=model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions on test set:"
      ],
      "metadata": {
        "id": "UzMBDpwjeqEz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSfCM3WkggMr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "test_dataloader = DataLoader(custom_test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "all_probabilities = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        inputs = {k: v.to(model.device) for k, v in batch.items() if k != 'labels' and k != 'token_type_ids'}\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.sigmoid(logits).cpu().numpy()\n",
        "        predicted_labels = (probabilities > 0.5).astype(float)\n",
        "\n",
        "        # Store predictions and probabilities\n",
        "        all_probabilities.extend(probabilities)\n",
        "        all_predictions.extend(predicted_labels)\n",
        "\n",
        "\n",
        "all_probabilities = np.array(all_probabilities)\n",
        "all_predictions = np.array(all_predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metric calculations:"
      ],
      "metadata": {
        "id": "i9auKoQTm_FL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score, recall_score, roc_auc_score\n",
        "\n",
        "#True labels:\n",
        "y_test = np.array([entry['labels'].numpy() for entry in encoded_dict['test']])\n",
        "\n",
        "#F1 Micro Average\n",
        "f1_micro = f1_score(y_test, all_predictions, average='micro')\n",
        "print(f\"F1 Micro Average: {f1_micro}\")\n",
        "\n",
        "#F1 Macro Average\n",
        "f1_macro = f1_score(y_test, all_predictions, average='macro')\n",
        "print(f\"F1 Macro Average: {f1_macro}\")\n",
        "\n",
        "#ROC AUC Micro\n",
        "roc_auc_micro = roc_auc_score(y_test, all_probabilities, average='micro')\n",
        "print(f\"ROC AUC Micro Average: {roc_auc_micro}\")\n",
        "\n",
        "#ROC AUC Macro\n",
        "roc_auc_macro = roc_auc_score(y_test, all_probabilities, average='macro')\n",
        "print(f\"ROC AUC Macro Average: {roc_auc_macro}\")\n",
        "\n",
        "#Hamming Score\n",
        "hamming_score = np.mean(y_test == all_predictions)\n",
        "print(f\"Hamming Score: {hamming_score}\")"
      ],
      "metadata": {
        "id": "lfyvi5MUe2Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting ROC AUC graphs:"
      ],
      "metadata": {
        "id": "XvwB_2BTm9Pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import interp1d\n",
        "from itertools import cycle\n",
        "\n",
        "n_classes = 8\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "y_test_flat = y_test.ravel()\n",
        "all_probabilities_flat = all_probabilities.ravel()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test_flat, all_probabilities_flat)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "unique_fpr, indices = np.unique(fpr, return_index=True)\n",
        "unique_tpr = tpr[indices]\n",
        "smooth_fpr = np.linspace(0, 1, 300)\n",
        "smooth_tpr = interp1d(unique_fpr, unique_tpr, kind='quadratic', fill_value=\"extrapolate\")(smooth_fpr)\n",
        "\n",
        "fpr, tpr = smooth_fpr, smooth_tpr\n",
        "\n",
        "\n",
        "roc_auc_direct = roc_auc_score(y_test_flat, all_probabilities_flat, average='micro')\n",
        "roc_auc_plot = auc(fpr, tpr)\n",
        "\n",
        "print(f\"ROC AUC from Plot Data: {roc_auc_plot}\")\n",
        "print(f\"ROC AUC Direct Calculation: {roc_auc_direct}\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.plot(fpr, tpr,\n",
        "         color='darkblue', linestyle='-', linewidth=3)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Micro-Average ROC Curve with Smoothing: Llama 2 7B')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KvuSrcJNR6Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per-class F1-scores & Accuracies:"
      ],
      "metadata": {
        "id": "q5wRQ1Jomp1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sRD-yuEhNF0"
      },
      "outputs": [],
      "source": [
        "def per_class_metrics(predictions, probabilities, labels):\n",
        "    class_metrics = {}\n",
        "    for i in range(labels.shape[1]):\n",
        "        class_f1 = f1_score(labels[:, i], predictions[:, i])\n",
        "        class_accuracy = accuracy_score(labels[:, i], predictions[:, i])\n",
        "        class_metrics[f\"Class_{i+1}\"] = {\"F1\": class_f1, \"Accuracy\": class_accuracy}\n",
        "    return class_metrics\n",
        "\n",
        "class_metrics = per_class_metrics(all_predictions, all_probabilities, y_test)\n",
        "\n",
        "for class_id, metrics in class_metrics.items():\n",
        "    print(f\"{class_id} - F1: {metrics['F1']}, Accuracy: {metrics['Accuracy']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrices for each label:"
      ],
      "metadata": {
        "id": "Pt23hHhSfE4x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHIwNwGXhgc5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "label_names = [\n",
        "    'Not Categorisable',\n",
        "    'Conflict & Crisis',\n",
        "    'Migration Flow',\n",
        "    'Host Country Security',\n",
        "    'Host Country Politics',\n",
        "    'Refugee Rights & Advocacy',\n",
        "    'Host Country Resources',\n",
        "    'Host Country Symbolic Discourse'\n",
        "]\n",
        "\n",
        "confusion_matrices = multilabel_confusion_matrix(y_test, all_predictions)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, label_name, class_names=['Negative', 'Positive']):\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title(f'Confusion Matrix for {label_name}')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for i, cm in enumerate(confusion_matrices):\n",
        "    plot_confusion_matrix(cm, label_names[i])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}