{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "ELzH1gh5HO0Y",
        "Y_pjM6-jHSFo",
        "7FmuiMDRIjac"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpqrG2vEENMl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate==0.20.3 torch==2.2.1\n",
        "! pip install -q -U transformers peft shap\n",
        "! pip install torch datasets\n",
        "! pip install tqdm"
      ],
      "metadata": {
        "id": "SXSeBKWBFvxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os,torch\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    Trainer\n",
        ")\n",
        "from peft import AutoPeftModelForSequenceClassification\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "_tAJ9ByQEvCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the final model:"
      ],
      "metadata": {
        "id": "G654NZBgf2Un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/My Drive/CBS/CBS Thesis Lydia & Sara/Models/mistral_0405'\n",
        "\n",
        "model = AutoPeftModelForSequenceClassification.from_pretrained(save_path,\n",
        "                                                               num_labels=8,\n",
        "                                                               problem_type=\"multi_label_classification\",\n",
        "                                                               use_auth_token='') #insert HF token\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "merged_model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "_rgL7R-LnoYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the data:"
      ],
      "metadata": {
        "id": "gs65NUeiPhUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/drive/MyDrive/CBS/CBS Thesis Lydia & Sara/Data/03_Data_Modeling/full_preprocessed_XX.csv' #adjust to current language\n",
        "df_temp= pd.read_csv(path, sep='\\t', encoding='utf-16')\n",
        "\n",
        "#Get 20 000 random rows for predictions\n",
        "df = df_temp.sample(n=20000, random_state=42)\n",
        "\n",
        "#Get 200 random rows for SHAP analysis\n",
        "df_shap = df_temp.sample(200, random_state=42)\n"
      ],
      "metadata": {
        "id": "j2D_TMx4HEFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create HF dataset:"
      ],
      "metadata": {
        "id": "ZJ9EOEwtPzHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN=512\n",
        "\n",
        "#Same set-up as in the Classification notebook\n",
        "class CustomHFDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_len, text_column, device=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dataset = dataset\n",
        "        self.text_column = text_column\n",
        "        self.max_len = max_len\n",
        "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        record = self.dataset[index]\n",
        "        text = str(record[self.text_column])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].flatten()\n",
        "        }\n",
        "\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "custom_dataset = CustomHFDataset(dataset, tokenizer,\n",
        "                                 max_len=MAX_LEN, text_column='Translation')"
      ],
      "metadata": {
        "id": "m8yw5LTarbBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform Predictions on the 20k sample:"
      ],
      "metadata": {
        "id": "Y_pjM6-jHSFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "gkaoJ2gbOusy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader = DataLoader(custom_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
        "        inputs = {k: v.to(model.device) for k, v in batch.items() if k != 'labels' and k != 'token_type_ids'}\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.sigmoid(logits).cpu().numpy()\n",
        "        predicted_labels = (probabilities > 0.5).astype(float)\n",
        "\n",
        "        # Store predictions and probabilities\n",
        "        all_probabilities.extend(probabilities)\n",
        "        all_predictions.extend(predicted_labels)\n",
        "\n",
        "all_probabilities = np.array(all_probabilities)\n",
        "all_predictions = np.array(all_predictions)\n",
        "list_of_predictions = all_predictions.tolist()"
      ],
      "metadata": {
        "id": "04orcOXFVVq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['predicted_label'] = list_of_predictions\n",
        "\n",
        "#Save the dataframe with the predictions-column\n",
        "save_path='/content/drive/MyDrive/CBS/CBS Thesis Lydia & Sara/Data/04_Data_Analysis/20k....csv' #change accordingly\n",
        "df.to_csv(save_path, sep='\\t', encoding='utf-16', index=False)"
      ],
      "metadata": {
        "id": "Dk3GhV4zg9lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHAP Analysis"
      ],
      "metadata": {
        "id": "-faMR-pTj77H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import pipeline\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "b6OeAfLKzLfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model=merged_model.to(device)\n",
        "merged_model.eval()"
      ],
      "metadata": {
        "id": "XmOyOzQSAQp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP calculations:"
      ],
      "metadata": {
        "id": "5ESwnSZSheYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_pipeline = pipeline(\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-classification\",\n",
        "    model=merged_model,\n",
        "    max_length=512,\n",
        "    return_all_scores=True,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "text_batches = [df_shap[\"Translation\"][i:i + batch_size] for i in range(0, df_shap[\"Translation\"].shape[0], batch_size)]\n",
        "\n",
        "explainer = shap.Explainer(inference_pipeline,\n",
        "                           inference_pipeline.tokenizer)\n",
        "\n",
        "shap_values_list = []\n",
        "\n",
        "for text_batch in tqdm(text_batches, desc=\"Explaining\"):\n",
        "    shap_values = explainer(text_batch.tolist())\n",
        "    shap_values_list.extend(shap_values)\n"
      ],
      "metadata": {
        "id": "GiRNb5zpHC7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the SHAP values\n",
        "\n",
        "file_path = '/content/drive/MyDrive/CBS/CBS Thesis Lydia & Sara/Data/04_Data_Analysis/SHAP_200_XX.pkl' #adjust accordingly\n",
        "\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(shap_values_list, file)\n",
        "\n",
        "print(\"SHAP values saved successfully.\")"
      ],
      "metadata": {
        "id": "_yIYsQtMQUff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting a shap plot from the shap_values_list\n",
        "shap.plots.text(shap_values_list[1])"
      ],
      "metadata": {
        "id": "stp67crWXZsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP analysis of all 1000 sampled tweets:"
      ],
      "metadata": {
        "id": "Ev4plGUZhbkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_SE= '/content/drive/MyDrive/CBS/CBS Thesis Lydia & Sara/Data/04_Data_Analysis/SHAP_200_SE.pkl'\n",
        "file_path_DA= '/content/drive/MyDrive/CBS/CBS Thesis Lydia & Sara/Data/04_Data_Analysis/DA_shap_values.pkl'\n",
        "file_path_FI= '/content/drive/MyDrive/CBS/CBS Thesis Lydia & Sara/Data/04_Data_Analysis/FI_shap_values.pkl'\n",
        "file_path_DE= '/content/drive/MyDrive/CBS/CBS Thesis Lydia & Sara/Data/04_Data_Analysis/DE_shap_values.pkl'\n",
        "file_path_EN= '/content/drive/MyDrive/CBS/CBS Thesis Lydia & Sara/Data/04_Data_Analysis/EN_shap_values.pkl'\n",
        "\n",
        "with open(file_path_DA, 'rb') as file:\n",
        "    DA_shap_values_loaded = pickle.load(file)\n",
        "\n",
        "with open(file_path_FI, 'rb') as file:\n",
        "    FI_shap_values_loaded = pickle.load(file)\n",
        "\n",
        "with open(file_path_SE, 'rb') as file:\n",
        "    SE_shap_values_loaded = pickle.load(file)\n",
        "\n",
        "with open(file_path_DE, 'rb') as file:\n",
        "    DE_shap_values_loaded = pickle.load(file)\n",
        "\n",
        "with open(file_path_EN, 'rb') as file:\n",
        "    EN_shap_values_loaded = pickle.load(file)\n",
        "\n",
        "all_shap_values = DA_shap_values_loaded + FI_shap_values_loaded + SE_shap_values_loaded + DE_shap_values_loaded + EN_shap_values_loaded"
      ],
      "metadata": {
        "id": "bucllJKZBeuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_names = [\n",
        "    'Not Categorisable',\n",
        "    'Conflict & Crisis',\n",
        "    'Migration Flow',\n",
        "    'Host Country Security',\n",
        "    'Host Country Politics',\n",
        "    'Refugee Rights & Advocacy',\n",
        "    'Host Country Resources',\n",
        "    'Host Country Symbolic Discourse'\n",
        "]\n",
        "\n",
        "# Dictionaries for the SHAP value sums and counts for each label\n",
        "token_shap_sums = defaultdict(lambda: defaultdict(float))\n",
        "token_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "# Collecting the SHAP values\n",
        "for shap_values in all_shap_values:\n",
        "    tokens = shap_values.data.flatten()\n",
        "    for label_idx, label_name in enumerate(label_names):\n",
        "        class_shap_values = shap_values.values[..., label_idx].flatten()\n",
        "        for token, shap_value in zip(tokens, class_shap_values):\n",
        "            token_shap_sums[token][label_name] += shap_value\n",
        "            token_counts[token][label_name] += 1\n",
        "\n",
        "total_token_counts = {token: sum(counts.values()) for token, counts in token_counts.items()}\n",
        "\n",
        "# Average SHAP value for each token per label (if the token has 3 or more occurences)\n",
        "token_info = {\n",
        "    token: {\n",
        "        label: token_shap_sums[token][label] / token_counts[token][label]\n",
        "        for label in label_names if token_counts[token][label] > 0\n",
        "    }\n",
        "    for token in token_counts if total_token_counts[token] >= 3\n",
        "}"
      ],
      "metadata": {
        "id": "SytvmDi83_Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_index = 7 #Choose class of interest\n",
        "\n",
        "target_label = label_names[class_index]\n",
        "\n",
        "# Sorting tokens by average SHAP value for the class_index label\n",
        "sorted_tokens = sorted(token_info.items(), key=lambda x: x[1].get(target_label, 0), reverse=True)\n",
        "\n",
        "#Take the top 10 of these\n",
        "N = 10\n",
        "top_tokens = sorted_tokens[:min(N, len(sorted_tokens))]\n",
        "\n",
        "\n",
        "data = {\n",
        "    'Token': [token for token, _ in top_tokens],\n",
        "    **{\n",
        "        label: [f\"{token_info[token].get(label, 'n/a'):.3f}\" if label in token_info[token] else 'n/a'\n",
        "                for token, _ in top_tokens]\n",
        "        for label in label_names\n",
        "    }\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.set_index('Token', inplace=True)"
      ],
      "metadata": {
        "id": "YfSQL-mwIePY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Top {len(top_tokens)} tokens by average SHAP value impact for '{target_label}':\")\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "uiGmy3dlTTs6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}